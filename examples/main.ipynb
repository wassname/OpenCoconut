{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from opencoconut import (\n",
    "    AutoCoconutForCausalLM,\n",
    "    CoconutConfig,\n",
    "    CoTDataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "from loguru import logger\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Qwen/Qwen2.5-0.5B',\n",
       " 'batch_size': 12,\n",
       " 'learning_rate': 0.0004,\n",
       " 'samples_per_epoch': 48,\n",
       " 'output_dir': './output/small',\n",
       " 'num_epochs': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_debug = {\n",
    "    'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 4e-4,\n",
    "    'samples_per_epoch': 48,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_small = {\n",
    "    'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 1e-4,\n",
    "    'samples_per_epoch': 2000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config_medium = {\n",
    "    'model_name': \"Qwen/Qwen2.5-2.5B\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'samples_per_epoch': 30000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config = config_small\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "    os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-14 20:23:30.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mInitializing model and tokenizer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "logger.info(\"Initializing model and tokenizer\")\n",
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "tokenizer.bos_token = \"<|im_start|>\"\n",
    "tokenizer.eos_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "# config and model\n",
    "coconut_config = CoconutConfig.from_tokenizer(\n",
    "    tokenizer,\n",
    "    stages=4,\n",
    "    continuous_thoughts=2,\n",
    ")\n",
    "model = AutoCoconutForCausalLM.from_pretrained(\n",
    "    config['model_name'], coconut_config, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if os.getenv(\"DEBUG\", \"0\") == \"1\":\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=config['samples_per_epoch']//config['batch_size']*config['num_epochs'],\n",
    "    logging_steps=100, # TODO ideally we log to tensorboard every step, but to ui every 100 steps\n",
    "    save_steps=10000,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    optim=\"adamw_torch\", # save memory: adamw_bnb_8bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base checkpoint\n",
    "if not DEBUG:\n",
    "    current_output_dir = output_dir/\"stage0}\"\n",
    "    current_output_dir = current_output_dir/\"checkpoint-base\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels']) torch.Size([12, 256])\n"
     ]
    }
   ],
   "source": [
    "# QC test\n",
    "clear_memory()\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=256, # all less than 256, most < 128\n",
    "    coconut_config=coconut_config,\n",
    "    current_stage=1,\n",
    "    split=f\"train[:{config['samples_per_epoch']}]\",\n",
    ")\n",
    "dl = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "batch = next(iter(dl))\n",
    "batch = {k: v.to(get_device()) for k, v in batch.items()}\n",
    "print(batch.keys(), batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "odict_keys(['loss', 'logits', 'past_key_values', 'hidden_states']) torch.Size([])\n",
      "3.0440244674682617\n",
      "Stage 1\n",
      "kv0 torch.Size([12, 2, 42, 64])\n",
      "kv.0-a torch.Size([12, 2, 42, 64]) 42\n"
     ]
    }
   ],
   "source": [
    "# QC test\n",
    "for i in [0, 1, 2]:\n",
    "    model.current_stage = i\n",
    "    print(f\"Stage {i}\")\n",
    "    with torch.no_grad():\n",
    "        # model.eval()\n",
    "        # o = model(**batch, output_hidden_states=True)\n",
    "        # print(o.keys())\n",
    "        model.train()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys(), o['loss'].shape)\n",
    "        print(o['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC test\n",
    "clear_memory()\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=256, # all less than 256, most < 128\n",
    "    coconut_config=coconut_config,\n",
    "    current_stage=1,\n",
    "    split=f\"train[:{config['samples_per_epoch']}]\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize trainer\n",
    "for stage in range(coconut_config.stages):\n",
    "    clear_memory()\n",
    "    logger.info(f\"starting stage {stage}\")\n",
    "    logger.info(\"preparing dataset\")\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=256, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=stage,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    logger.info(f\"dataset size: {len(dataset)}\")\n",
    "    model.current_stage = stage\n",
    "    current_output_dir = output_dir/f\"stage{stage}\"\n",
    "    training_args.output_dir = current_output_dir\n",
    "\n",
    "    if stage == 0:\n",
    "        training_args.num_train_epochs = config['num_epochs']\n",
    "    elif stage == coconut_config.stages-2:\n",
    "        # Penultimate stage removes all the remaining language reasoning chain\n",
    "        # This handles the long-tail distribution of reasoning chains longer than 3 steps\n",
    "        dataset.include_reasoning_steps = False\n",
    "        training_args.num_train_epochs = config['num_epochs']\n",
    "    elif stage == coconut_config.stages-1:\n",
    "        # For all datasets, after the standard schedule,\n",
    "        # the model stays in the final training stage, until the 50th epoch.\n",
    "        dataset.include_reasoning_steps = True\n",
    "        training_args.num_train_epochs = config['num_epochs'] * 3\n",
    "\n",
    "    logger.info(\"starting training\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # save tokenizer to all checkpoints after training\n",
    "    for folder in os.listdir(current_output_dir):\n",
    "        if folder.startswith(\"checkpoint-\"):\n",
    "            checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "            if os.path.isdir(checkpoint_folder):\n",
    "                tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "    # save final checkpoint\n",
    "    current_output_dir = current_output_dir/\"checkpoint-final\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)\n",
    "    logger.info(f\"finished stage {stage}. Saved to {current_output_dir}\")\n",
    "\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "prompt = \"John cuts his grass to 2 inches. \" \\\n",
    "         \"It grows .5 inches per month. \" \\\n",
    "         \"When it gets to 4 inches he cuts it back down to 2 inches. \" \\\n",
    "         \"It cost $100 to get his grass cut. How much does he pay per year?\"\n",
    "\n",
    "ans = \"\"\"\n",
    "# since it starts at 2 and never gets cut below 2, we can consider only the extra growth\n",
    "growth_annual = 0.5*12\n",
    "cost_per_inch = 100/2\n",
    "cuts = growth_annual // 2 # round it down\n",
    "cost_per_year = growth_annual * cost_per_inch\n",
    "print(f\"cost per year: {cost_per_year}==300.0\")\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print('## With out thought token')\n",
    "outputs1 = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=64,\n",
    "    streamer=streamer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# add beginning of thought token?\n",
    "inputs['input_ids'], inputs['attention_mask'] = model.append_bot(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print('\\n## With thought token')\n",
    "outputs2 = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=64,\n",
    "    streamer=streamer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_generated_answer(model_output: str, eos_token=\"<|im_end|>\"):\n",
    "    answer_prefix = \"Answer: \"\n",
    "    start_index = model_output.find(answer_prefix)\n",
    "\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    start_index += len(answer_prefix)\n",
    "    end_index = model_output.find(eos_token, start_index)\n",
    "\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    extracted_answer = model_output[start_index:end_index].strip()\n",
    "    return extracted_answer\n",
    "\n",
    "def pp(s):\n",
    "    s = s.replace(tokenizer.eos_token, '')\n",
    "    s = s.replace(tokenizer.bos_token, '')\n",
    "    s = s.replace(tokenizer.pad_token, '')\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    verbose = 1,\n",
    "    add_bot = False\n",
    "):\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        if add_bot:\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"] = model.append_bot(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        (\n",
    "            thought_ids,\n",
    "            language_ids,\n",
    "            thought_mask,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = split_sequences(**batch, coconut_config=model.coconut_config)\n",
    "        batch_size = thought_ids.shape[0]\n",
    "        total_instances += batch_size\n",
    "\n",
    "        # Generate\n",
    "        beam_output = model.generate(\n",
    "            input_ids=thought_ids.to(model.device),\n",
    "            attention_mask=thought_mask.to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Evaluate\n",
    "        for thought_ids_batch, output_batch in zip(thought_ids, beam_output):\n",
    "            decoded_language_ids = tokenizer.decode(language_ids[0])\n",
    "            decoded_pred_text = tokenizer.decode(output_batch)\n",
    "            answer = extract_generated_answer(\n",
    "                decoded_language_ids, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            pred_answer = extract_generated_answer(\n",
    "                decoded_pred_text, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            if answer == pred_answer:\n",
    "                total_correct += 1\n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f\"Input: {pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}\\n\"\n",
    "                    f\"decoded_language_ids: {pp(decoded_language_ids)}\\n\"\n",
    "                    f\"decoded_pred_text: {pp(decoded_pred_text)}\\n\"\n",
    "                    f\"Target: {pp(answer)}\\n\"\n",
    "                    f\"Predicted: {pp(pred_answer)}\\n\"\n",
    "                )\n",
    "    if verbose>0:\n",
    "        print(\n",
    "            f\"Input: {pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}\\n\"\n",
    "            f\"decoded_language_ids: {pp(decoded_language_ids)}\\n\"\n",
    "            f\"decoded_pred_text: {pp(decoded_pred_text)}\\n\"\n",
    "            f\"Target: {pp(answer)}\\n\"\n",
    "            f\"Predicted: {pp(pred_answer)}\\n\"\n",
    "        )\n",
    "    accuracy = total_correct / total_instances\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "# Load data\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=max_new_tokens,\n",
    "    coconut_config=model.coconut_config,\n",
    "    current_stage=model.coconut_config.stages,\n",
    "    split=\"valid\",\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "# eval final model\n",
    "accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we explicitly start of BoThough token\n",
    "accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens, add_bot=True)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval all check\n",
    "\n",
    "checkpoints = sorted(output_dir.glob(\"stage*/*base\")) + sorted(output_dir.glob(\"stage*/*final\"))\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Loading checkpoint: {checkpoint}\")\n",
    "    model = AutoCoconutForCausalLM.from_pretrained(\n",
    "        checkpoint, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    "    ).eval()\n",
    "    model.tokenizer = tokenizer\n",
    "    accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens)\n",
    "    print(f\"Checkpoint: {checkpoint}, Accuracy: {accuracy}\")\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
