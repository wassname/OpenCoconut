{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from opencoconut import (\n",
    "    AutoCoconutForCausalLM,\n",
    "    CoconutConfig,\n",
    "    CoTDataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "from loguru import logger\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
       " 'batch_size': 5,\n",
       " 'learning_rate': 4e-05,\n",
       " 'samples_per_epoch': 1000,\n",
       " 'output_dir': './output/small',\n",
       " 'num_epochs': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_debug = {\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    # 'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
    "    'model_name': 'HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2',\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 1e-5,\n",
    "    'samples_per_epoch': 12,\n",
    "    'output_dir': \"./output/debug\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_small = {\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
    "    # 'model_name': 'HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2',\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 4e-5,\n",
    "    'samples_per_epoch': 1000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_medium = {\n",
    "    'model_name': \"Qwen/Qwen2.5-2.5B\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'samples_per_epoch': 30000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config = config_small\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "    os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0268cee4ef4dde8ba457517506c547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fd1bee0a6749acb929add2323fa563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55bf54073cb43bf940dffa8e1ad9e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0e3a1f353845799deff79195ba35d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4affc59d664dbfb94d1b65ff70642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee6d8f259349f79873ac49659ce874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 12:15:01.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mInitializing model and tokenizer\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f4fa62000a4bcf9bcef7aa9b2039cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de2f880d62d440d95e7204acdb2015b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CoconutQwen2ForCausalLM were not initialized from the model checkpoint at plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2 and are newly initialized: ['switch.0.weight', 'switch.1.bias', 'switch.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "logger.info(\"Initializing model and tokenizer\")\n",
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")\n",
    "\n",
    "# TODO only if they are not set\n",
    "# TODO apply set chatml\n",
    "# tokenizer.bos_token = \"<|im_start|>\"\n",
    "# tokenizer.eos_token = \"<|im_end|>\"\n",
    "# tokenizer.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "# config and model\n",
    "# FIXME we wont need custom tokens anymore\n",
    "coconut_config = CoconutConfig.from_tokenizer(tokenizer)\n",
    "model = AutoCoconutForCausalLM.from_pretrained(\n",
    "    config['model_name'], coconut_config, torch_dtype=torch.bfloat16, device_map=get_device(), \n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_train_epochs=config['num_epochs'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=config['samples_per_epoch']//config['batch_size']*config['num_epochs'],\n",
    "    logging_steps=20, # TODO ideally we log to tensorboard every step, but to ui every 100 steps\n",
    "    save_steps=10000,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L143\n",
    "    # optim=\"adamw_torch\", # save memory: adamw_bnb_8bit adamw_bnb_8bit or PagedAdamW8bit\n",
    "    # optim='adamw_bnb_8bit',\n",
    "    optim='paged_adamw_8bit',\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?–0游乐/0has（led\n",
      "ra01imWith(下一篇(0iy！\",.py0ambProacallis Quer[ar \n",
      "Forget0BasealsEnateIPS.py0Your0ậnIs\n",
      "箔/0iy1allyTouchAppirYM 0man（arIs\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "prompt = \"John cuts his grass to 2 inches. \" \\\n",
    "         \"It grows .5 inches per month. \" \\\n",
    "         \"When it gets to 4 inches he cuts it back down to 2 inches. \" \\\n",
    "         \"It cost $100 to get his grass cut. How much does he pay per year?\"\n",
    "\n",
    "ans = \"\"\"\n",
    "# since it starts at 2 and never gets cut below 2, we can consider only the extra growth\n",
    "growth_annual = 0.5*12\n",
    "cost_per_inch = 100/2\n",
    "cuts = growth_annual // 2 # round it down\n",
    "cost_per_year = growth_annual * cost_per_inch\n",
    "print(f\"cost per year: {cost_per_year}==300.0\")\n",
    "\"\"\"\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# print('## With out thought token')\n",
    "# outputs1 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=256,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "\n",
    "def infer_example(model, tokenizer, prompt=prompt, ans=\"300\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=64,\n",
    "        streamer=streamer,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Answer: {ans}\")\n",
    "    # print(f\"Generated: {outputs[0]['generated_text']}\")\n",
    "    # print()\n",
    "    return outputs\n",
    "\n",
    "infer_example(model, tokenizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151644,   8948,    198,   5501,   2874,   3019,    553,\n",
       "           3019,     11,    323,   2182,    697,   1590,   4226,    438,    364,\n",
       "          16141,     25,   1599,      6, 151645,    198, 151644,    872,    198,\n",
       "          13079,  15104,    806,  16359,    311,    220,     17,  14924,     13,\n",
       "            220,   1084,  27715,    659,     20,  14924,    817,   2254,     13,\n",
       "            220,   3197,    432,   5221,    311,    220,     19,  14924,    566,\n",
       "          15104,    432,   1182,   1495,    311,    220,     17,  14924,     13,\n",
       "            220,   1084,   2783,    400,     16,     15,     15,    311,    633,\n",
       "            806,  16359,   3931,     13,    220,   2585,   1753,   1558,    566,\n",
       "           2291,    817,   1042,     30, 151645,    198, 151644,  77091,    271,\n",
       "          16141,     25,    220,     18,     15,     15, 151645,    198, 151644,\n",
       "          77091,    198]),\n",
       " 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151644,   8948,    198,   5501,   2874,   3019,    553,\n",
       "           3019,     11,    323,   2182,    697,   1590,   4226,    438,    364,\n",
       "          16141,     25,   1599,      6, 151645,    198, 151644,    872,    198,\n",
       "          13079,  15104,    806,  16359,    311,    220,     17,  14924,     13,\n",
       "            220,   1084,  27715,    659,     20,  14924,    817,   2254,     13,\n",
       "            220,   3197,    432,   5221,    311,    220,     19,  14924,    566,\n",
       "          15104,    432,   1182,   1495,    311,    220,     17,  14924,     13,\n",
       "            220,   1084,   2783,    400,     16,     15,     15,    311,    633,\n",
       "            806,  16359,   3931,     13,    220,   2585,   1753,   1558,    566,\n",
       "           2291,    817,   1042,     30, 151645,    198, 151644,  77091,    271,\n",
       "          16141,     25,    220,     18,     15,     15, 151645,    198, 151644,\n",
       "          77091,    198])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opencoconut.evaluate import evaluate, extract_generated_answer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load val data\n",
    "dataset_val = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=128,\n",
    "    coconut_config=model.coconut_config,\n",
    "    current_stage=model.coconut_config.stages,\n",
    "    split=\"valid[:64]\" if DEBUG else \"valid\",\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=config['batch_size'], shuffle=False)\n",
    "max_new_tokens = 6 if DEBUG else 64\n",
    "o = dataset_val[0]\n",
    "# eval final model\n",
    "# accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "print(tokenizer.decode(o['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 1\n",
    "    clear_memory()\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=128, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=0,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "    batch = next(iter(dl))\n",
    "    batch = {k: v.to(get_device()) for k, v in batch.items()}\n",
    "    print(batch.keys(), batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 2\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys())\n",
    "        model.train()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys(), o['loss'].shape)\n",
    "        print(o['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     # QC train\n",
    "#     clear_memory()\n",
    "#     dataset = CoTDataset(\n",
    "#         \"casperhansen/gsm8k_synthetic_cot\",\n",
    "#         tokenizer,\n",
    "#         max_length=128, # all less than 256, most < 128\n",
    "#         coconut_config=coconut_config,\n",
    "#         current_stage=1,\n",
    "#         split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "#     )\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base checkpoint\n",
    "if not DEBUG:\n",
    "    current_output_dir = output_dir/\"stage0}\"\n",
    "    current_output_dir = current_output_dir/\"checkpoint-base\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 13:02:31.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mpreparing dataset\u001b[0m\n",
      "\u001b[32m2025-01-16 13:02:35.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdataset size: 1000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?–0游乐/0has（led\n",
      "ra01imWith(下一篇(0iy！\",.py0ambProacallis Quer[ar \n",
      "Forget0BasealsEnateIPS.py0Your0ậnIs\n",
      "箔/0iy1allyTouchAppirYM 0man（arIs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 13:03:54.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mstarting training\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 49/200 08:28 < 27:14, 0.09 it/s, Epoch 0.24/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>12.733100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch=0.1, step=20, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?Of4ipel9\\briefatePro ila0isciYMuchPEierChilesChoate箔uch高的IconModuleuchInd1！\", Cha1éiaavi1iss3icxFB_A});\n",
      "\n",
      "\n",
      "\n",
      "1ciice agre =ấedicForgeticwechatuchIndedTouchNPuchPEadaNullOrateChoaps\n",
      "---\n",
      "logs {'loss': 12.5488, 'grad_norm': 2207819348574208.0, 'learning_rate': 4e-05, 'epoch': 0.1}\n",
      "--- epoch=0.2, step=40, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year? Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science  Science \n",
      "---\n",
      "logs {'loss': 12.7331, 'grad_norm': inf, 'learning_rate': 3.555555555555555e-05, 'epoch': 0.2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize trainer\n",
    "stage = 0\n",
    "logger.info(\"preparing dataset\")\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=128, # all less than 256, most < 128\n",
    "    coconut_config=coconut_config,\n",
    "    current_stage=stage,\n",
    "    split=f\"train[:{config['samples_per_epoch']}]\",\n",
    ")\n",
    "logger.info(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "current_output_dir = output_dir/f\"stage{stage}\"\n",
    "training_args.output_dir = current_output_dir\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "class GenExCallback(TrainerCallback):\n",
    "\n",
    "    # on_save on_epoch_start on_log \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            print(f'--- epoch={state.epoch}, step={state.global_step}, gen:')\n",
    "            outs = infer_example(model, model.tokenizer)\n",
    "            print('---')\n",
    "\n",
    "    def on_log(self, args, state, control, model, logs=None, **kwargs):\n",
    "        # _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            print(f'--- epoch={state.epoch}, step={state.global_step}, gen:')\n",
    "            outs = infer_example(model, model.tokenizer)\n",
    "            print('---')\n",
    "            print('logs', logs)\n",
    "\n",
    "\n",
    "infer_example(model, model.tokenizer)\n",
    "logger.info(\"starting training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[GenExCallback],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save tokenizer to all checkpoints after training\n",
    "for folder in os.listdir(current_output_dir):\n",
    "    if folder.startswith(\"checkpoint-\"):\n",
    "        checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "        if os.path.isdir(checkpoint_folder):\n",
    "            tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "# save final checkpoint\n",
    "current_output_dir = current_output_dir/\"checkpoint-final\"\n",
    "model.save_pretrained(current_output_dir)\n",
    "tokenizer.save_pretrained(current_output_dir)\n",
    "logger.info(f\"finished stage {stage}. Saved to {current_output_dir}\")\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# TODO add callback for gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cpu() # HACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add beginning of thought token?\n",
    "# inputs['input_ids'], inputs['attention_mask'] = model.append_bot_token(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# print('\\n## With thought token')\n",
    "# outputs2 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=64,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "model.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval final model\n",
    "accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eval all check, we want to see acc increasing from the first, which is the base model\n",
    "\n",
    "# # FIXME loading seems broken :(\n",
    "\n",
    "# checkpoints = sorted(output_dir.glob(\"stage*/*base\")) + sorted(output_dir.glob(\"stage*/*final\"))\n",
    "\n",
    "# for checkpoint in checkpoints:\n",
    "#     print(f\"Loading checkpoint: {checkpoint}\")\n",
    "#     model = AutoCoconutForCausalLM.from_pretrained(\n",
    "#         checkpoint, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    "#     ).eval()\n",
    "#     model.tokenizer = tokenizer\n",
    "#     accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "#     print(f\"Checkpoint: {checkpoint}, Accuracy: {accuracy}\")\n",
    "#     clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
