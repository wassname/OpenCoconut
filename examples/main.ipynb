{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from opencoconut import (\n",
    "    AutoCoconutForCausalLM,\n",
    "    CoconutConfig,\n",
    "    CoTDataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "from loguru import logger\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2',\n",
       " 'batch_size': 6,\n",
       " 'learning_rate': 4e-05,\n",
       " 'samples_per_epoch': 1000,\n",
       " 'output_dir': './output/small',\n",
       " 'num_epochs': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_debug = {\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
    "    # 'model_name': 'HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2',\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 1e-5,\n",
    "    'samples_per_epoch': 12,\n",
    "    'output_dir': \"./output/debug\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_small = {\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    # 'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.2',\n",
    "    'model_name': 'HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2',\n",
    "    'batch_size': 6,\n",
    "    'learning_rate': 4e-5,\n",
    "    'samples_per_epoch': 1000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_medium = {\n",
    "    'model_name': \"Qwen/Qwen2.5-2.5B\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'samples_per_epoch': 30000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config = config_small\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "    os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'Please reason step by step, and put your final answer within \\\\\\\\boxed{}.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{}.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 13:40:01.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mInitializing model and tokenizer\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954d94de500a4cc8bed6c075cc717043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   2%|1         | 52.4M/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CoconutQwen2ForCausalLM were not initialized from the model checkpoint at HuggingFaceH4/Qwen2.5-Math-1.5B-Instruct-PRM-0.2 and are newly initialized: ['switch.0.weight', 'switch.1.bias', 'switch.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "logger.info(\"Initializing model and tokenizer\")\n",
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")\n",
    "\n",
    "# TODO only if they are not set\n",
    "# TODO apply set chatml\n",
    "# tokenizer.bos_token = \"<|im_start|>\"\n",
    "# tokenizer.eos_token = \"<|im_end|>\"\n",
    "# tokenizer.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "# config and model\n",
    "# FIXME we wont need custom tokens anymore\n",
    "coconut_config = CoconutConfig.from_tokenizer(tokenizer)\n",
    "model = AutoCoconutForCausalLM.from_pretrained(\n",
    "    config['model_name'], coconut_config, torch_dtype=torch.bfloat16, device_map=get_device(), \n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_train_epochs=config['num_epochs'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=config['samples_per_epoch']//config['batch_size']*config['num_epochs'],\n",
    "    logging_steps=20, # TODO ideally we log to tensorboard every step, but to ui every 100 steps\n",
    "    save_steps=10000,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L143\n",
    "    # optim=\"adamw_torch\", # save memory: adamw_bnb_8bit adamw_bnb_8bit or PagedAdamW8bit\n",
    "    # optim='adamw_bnb_8bit',\n",
    "    optim='paged_adamw_8bit',\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "prompt = \"John cuts his grass to 2 inches. \" \\\n",
    "         \"It grows .5 inches per month. \" \\\n",
    "         \"When it gets to 4 inches he cuts it back down to 2 inches. \" \\\n",
    "         \"It cost $100 to get his grass cut. How much does he pay per year?\"\n",
    "\n",
    "ans = \"\"\"\n",
    "# since it starts at 2 and never gets cut below 2, we can consider only the extra growth\n",
    "growth_annual = 0.5*12\n",
    "cost_per_inch = 100/2\n",
    "cuts = growth_annual // 2 # round it down\n",
    "cost_per_year = growth_annual * cost_per_inch\n",
    "print(f\"cost per year: {cost_per_year}==300.0\")\n",
    "\"\"\"\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# print('## With out thought token')\n",
    "# outputs1 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=256,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "\n",
    "def infer_example(model, tokenizer, prompt=prompt, ans=\"300\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=64,\n",
    "        streamer=streamer,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Answer: {ans}\")\n",
    "    # print(f\"Generated: {outputs[0]['generated_text']}\")\n",
    "    # print()\n",
    "    return outputs\n",
    "\n",
    "infer_example(model, tokenizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>system\n",
      "Please reason step by step, and put your final answer as 'Answer: X'<|im_end|>\n",
      "<|im_start|>user\n",
      "John cuts his grass to 2 inches.  It grows .5 inches per month.  When it gets to 4 inches he cuts it back down to 2 inches.  It cost $100 to get his grass cut.  How much does he pay per year?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Answer: 300<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from opencoconut.evaluate import evaluate, extract_generated_answer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load val data\n",
    "dataset_val = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=128,\n",
    "    coconut_config=model.coconut_config,\n",
    "    current_stage=model.coconut_config.stages,\n",
    "    split=\"valid[:64]\" if DEBUG else \"valid\",\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=config['batch_size'], shuffle=False)\n",
    "max_new_tokens = 6 if DEBUG else 64\n",
    "o = dataset_val[0]\n",
    "# eval final model\n",
    "# accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "print(tokenizer.decode(o['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 1\n",
    "    clear_memory()\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=128, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=0,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "    batch = next(iter(dl))\n",
    "    batch = {k: v.to(get_device()) for k, v in batch.items()}\n",
    "    print(batch.keys(), batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 2\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys())\n",
    "        model.train()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys(), o['loss'].shape)\n",
    "        print(o['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     # QC train\n",
    "#     clear_memory()\n",
    "#     dataset = CoTDataset(\n",
    "#         \"casperhansen/gsm8k_synthetic_cot\",\n",
    "#         tokenizer,\n",
    "#         max_length=128, # all less than 256, most < 128\n",
    "#         coconut_config=coconut_config,\n",
    "#         current_stage=1,\n",
    "#         split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "#     )\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base checkpoint\n",
    "if not DEBUG:\n",
    "    current_output_dir = output_dir/\"stage0}\"\n",
    "    current_output_dir = current_output_dir/\"checkpoint-base\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 13:44:10.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mpreparing dataset\u001b[0m\n",
      "\u001b[32m2025-01-16 13:44:14.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mdataset size: 1000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "[\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 13:45:47.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mstarting training\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166/166 1:02:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.657700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.587700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch=0.11976047904191617, step=20, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "---\n",
      "logs {'loss': 12.3171, 'grad_norm': 177.0, 'learning_rate': 3.9194630872483226e-05, 'epoch': 0.11976047904191617}\n",
      "--- epoch=0.23952095808383234, step=40, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "---\n",
      "logs {'loss': 6.946, 'grad_norm': 32.25, 'learning_rate': 3.38255033557047e-05, 'epoch': 0.23952095808383234}\n",
      "--- epoch=0.3592814371257485, step=60, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 5.8091, 'grad_norm': 8.625, 'learning_rate': 2.8456375838926177e-05, 'epoch': 0.3592814371257485}\n",
      "--- epoch=0.47904191616766467, step=80, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 4.3013, 'grad_norm': 7.8125, 'learning_rate': 2.3087248322147656e-05, 'epoch': 0.47904191616766467}\n",
      "--- epoch=0.5988023952095808, step=100, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 3.991, 'grad_norm': 10.3125, 'learning_rate': 1.771812080536913e-05, 'epoch': 0.5988023952095808}\n",
      "--- epoch=0.718562874251497, step=120, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 3.8381, 'grad_norm': 13.375, 'learning_rate': 1.2348993288590606e-05, 'epoch': 0.718562874251497}\n",
      "--- epoch=0.8383233532934131, step=140, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 3.6577, 'grad_norm': 14.3125, 'learning_rate': 6.979865771812081e-06, 'epoch': 0.8383233532934131}\n",
      "--- epoch=0.9580838323353293, step=160, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'loss': 3.5877, 'grad_norm': 13.5625, 'learning_rate': 1.6107382550335572e-06, 'epoch': 0.9580838323353293}\n",
      "--- epoch=0.9940119760479041, step=166, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "--- epoch=0.9940119760479041, step=166, gen:\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?0000000000000000000000000000000000000000000000000000000000000000\n",
      "---\n",
      "logs {'train_runtime': 3639.7871, 'train_samples_per_second': 0.274, 'train_steps_per_second': 0.046, 'total_flos': 1002353843579904.0, 'train_loss': 5.495260342057929, 'epoch': 0.9940119760479041}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 14:48:52.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mfinished stage 0. Saved to output/small/stage0/checkpoint-final\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize trainer\n",
    "stage = 0\n",
    "logger.info(\"preparing dataset\")\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=128, # all less than 256, most < 128\n",
    "    coconut_config=coconut_config,\n",
    "    current_stage=stage,\n",
    "    split=f\"train[:{config['samples_per_epoch']}]\",\n",
    ")\n",
    "logger.info(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "current_output_dir = output_dir/f\"stage{stage}\"\n",
    "training_args.output_dir = current_output_dir\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "class GenExCallback(TrainerCallback):\n",
    "\n",
    "    # on_save on_epoch_start on_log \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            print(f'--- epoch={state.epoch}, step={state.global_step}, gen:')\n",
    "            outs = infer_example(model, model.tokenizer)\n",
    "            print('---')\n",
    "\n",
    "    def on_log(self, args, state, control, model, logs=None, **kwargs):\n",
    "        # _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            print(f'--- epoch={state.epoch}, step={state.global_step}, gen:')\n",
    "            outs = infer_example(model, model.tokenizer)\n",
    "            print('---')\n",
    "            print('logs', logs)\n",
    "\n",
    "\n",
    "infer_example(model, model.tokenizer)\n",
    "logger.info(\"starting training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[GenExCallback],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save tokenizer to all checkpoints after training\n",
    "for folder in os.listdir(current_output_dir):\n",
    "    if folder.startswith(\"checkpoint-\"):\n",
    "        checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "        if os.path.isdir(checkpoint_folder):\n",
    "            tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "# save final checkpoint\n",
    "current_output_dir = current_output_dir/\"checkpoint-final\"\n",
    "model.save_pretrained(current_output_dir)\n",
    "tokenizer.save_pretrained(current_output_dir)\n",
    "logger.info(f\"finished stage {stage}. Saved to {current_output_dir}\")\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# TODO add callback for gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cpu() # HACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add beginning of thought token?\n",
    "# inputs['input_ids'], inputs['attention_mask'] = model.append_bot_token(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# print('\\n## With thought token')\n",
    "# outputs2 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=64,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "model.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def extract_generated_answer(model_output: str, eos_token=\"<|im_end|>\"):\n",
    "    answer_prefix = \"Answer: \"\n",
    "    start_index = model_output.find(answer_prefix)\n",
    "\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    start_index += len(answer_prefix)\n",
    "    end_index = model_output.find(eos_token, start_index)\n",
    "\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    extracted_answer = model_output[start_index:end_index].strip()\n",
    "    return extracted_answer\n",
    "\n",
    "def _pp(s, tokenizer):\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = s.replace(tokenizer.eos_token, '')\n",
    "    s = s.replace(tokenizer.bos_token, '')\n",
    "    s = s.replace(tokenizer.pad_token, '')\n",
    "    return s\n",
    "\n",
    "\n",
    "# def generate(\n",
    "#     model,\n",
    "#     input_ids,\n",
    "#     attention_mask,\n",
    "#     max_new_tokens=1,\n",
    "#     eos_token_id=None,\n",
    "#     ):\n",
    "#     # Generate one token at a time using kv_cache\n",
    "\n",
    "#     i = 0\n",
    "#     while i < max_new_tokens:\n",
    "#         beam_output = model.forward(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             max_new_tokens=1,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             pad_token_id=model.tokenizer.pad_token_id,\n",
    "#             # use_cache=False,\n",
    "#         )\n",
    "#         i += 1\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    verbose = 1,\n",
    "    add_bot = False\n",
    "):\n",
    "    pp = functools.partial(_pp, tokenizer=tokenizer)\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        # if add_bot:\n",
    "        #     batch[\"input_ids\"], batch[\"attention_mask\"] = model.append_bot_token(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        # (\n",
    "        #     thought_ids,\n",
    "        #     language_ids,\n",
    "        #     thought_mask,\n",
    "        #     _,\n",
    "        #     _,\n",
    "        #     _,\n",
    "        # ) = split_sequences(**batch, coconut_config=model.coconut_config)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        total_instances += batch_size\n",
    "\n",
    "        # Generate \n",
    "        beam_output = model.generate(\n",
    "            input_ids=input_ids.to(model.device),\n",
    "            attention_mask=attention_mask.to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            # use_cache=False,\n",
    "        )\n",
    "        # Evaluate\n",
    "        for thought_ids_batch, output_batch in zip(input_ids, beam_output):\n",
    "            decoded_language_ids = tokenizer.decode(language_ids[0])\n",
    "            decoded_pred_text = tokenizer.decode(output_batch)\n",
    "            answer = extract_generated_answer(\n",
    "                decoded_language_ids, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            pred_answer = extract_generated_answer(\n",
    "                decoded_pred_text, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            if answer == pred_answer:\n",
    "                total_correct += 1\n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f\"Input: `{pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}`\\n\"\n",
    "                    f\"decoded_language_ids: `{pp(decoded_language_ids)}`\\n\"\n",
    "                    f\"decoded_pred_text: `{pp(decoded_pred_text)}`\\n\"\n",
    "                    f\"Target: `{pp(answer)}`\\n\"\n",
    "                    f\"Predicted: `{pp(pred_answer)}`\\n\"\n",
    "                )\n",
    "    if verbose>0:\n",
    "        print(\n",
    "            f\"Input: `{pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}`\\n\"\n",
    "            f\"decoded_language_ids: `{pp(decoded_language_ids)}`\\n\"\n",
    "            f\"decoded_pred_text: `{pp(decoded_pred_text)}`\\n\"\n",
    "            f\"Target: `{pp(answer)}`\\n\"\n",
    "            f\"Predicted: `{pp(pred_answer)}`\\n\"\n",
    "        )\n",
    "    accuracy = total_correct / total_instances\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d981543c49466483b24a3d54c143ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'language_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# eval final model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 100\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, tokenizer, model, max_new_tokens, verbose, add_bot)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thought_ids_batch, output_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_ids, beam_output):\n\u001b[0;32m--> 100\u001b[0m     decoded_language_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mlanguage_ids\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    101\u001b[0m     decoded_pred_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_batch)\n\u001b[1;32m    102\u001b[0m     answer \u001b[38;5;241m=\u001b[39m extract_generated_answer(\n\u001b[1;32m    103\u001b[0m         decoded_language_ids, eos_token\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'language_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# eval final model\n",
    "accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eval all check, we want to see acc increasing from the first, which is the base model\n",
    "\n",
    "# # FIXME loading seems broken :(\n",
    "\n",
    "# checkpoints = sorted(output_dir.glob(\"stage*/*base\")) + sorted(output_dir.glob(\"stage*/*final\"))\n",
    "\n",
    "# for checkpoint in checkpoints:\n",
    "#     print(f\"Loading checkpoint: {checkpoint}\")\n",
    "#     model = AutoCoconutForCausalLM.from_pretrained(\n",
    "#         checkpoint, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    "#     ).eval()\n",
    "#     model.tokenizer = tokenizer\n",
    "#     accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "#     print(f\"Checkpoint: {checkpoint}, Accuracy: {accuracy}\")\n",
    "#     clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
