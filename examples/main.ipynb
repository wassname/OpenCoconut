{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from opencoconut import (\n",
    "    AutoCoconutForCausalLM,\n",
    "    CoconutConfig,\n",
    "    CoTDataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "from loguru import logger\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Qwen/Qwen2.5-0.5B',\n",
       " 'batch_size': 12,\n",
       " 'learning_rate': 0.0004,\n",
       " 'samples_per_epoch': 48,\n",
       " 'output_dir': './output/small',\n",
       " 'num_epochs': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_debug = {\n",
    "    'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 4e-4,\n",
    "    'samples_per_epoch': 48,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_small = {\n",
    "    'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'batch_size': 12,\n",
    "    'learning_rate': 1e-4,\n",
    "    'samples_per_epoch': 2000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config_medium = {\n",
    "    'model_name': \"Qwen/Qwen2.5-2.5B\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'samples_per_epoch': 30000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config = config_small\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "    os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-15 14:55:37.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mInitializing model and tokenizer\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.48 GiB of which 4.31 MiB is free. Process 1079651 has 22.64 GiB memory in use. Including non-PyTorch memory, this process has 820.00 MiB memory in use. Of the allocated memory 547.57 MiB is allocated by PyTorch, and 18.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# config and model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m coconut_config \u001b[38;5;241m=\u001b[39m CoconutConfig\u001b[38;5;241m.\u001b[39mfrom_tokenizer(\n\u001b[1;32m     13\u001b[0m     tokenizer,\n\u001b[1;32m     14\u001b[0m     stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     15\u001b[0m     continuous_thoughts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoCoconutForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoconut_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/opencoconut/models/_auto.py:44\u001b[0m, in \u001b[0;36mAutoCoconutForCausalLM.from_pretrained\u001b[0;34m(cls, model_path, config, config_init_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m model_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs)\n\u001b[1;32m     42\u001b[0m model_config\u001b[38;5;241m.\u001b[39mcoconut_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCOCONUT_CAUSAL_LM_MODEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4224\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4214\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4216\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4217\u001b[0m         (\n\u001b[1;32m   4218\u001b[0m             model,\n\u001b[1;32m   4219\u001b[0m             missing_keys,\n\u001b[1;32m   4220\u001b[0m             unexpected_keys,\n\u001b[1;32m   4221\u001b[0m             mismatched_keys,\n\u001b[1;32m   4222\u001b[0m             offload_index,\n\u001b[1;32m   4223\u001b[0m             error_msgs,\n\u001b[0;32m-> 4224\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4244\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4245\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4794\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4792\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4793\u001b[0m         fixed_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4794\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4796\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfixed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4797\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4800\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4801\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4802\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4803\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4804\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4805\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4806\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4807\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4808\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4810\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4812\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:873\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    870\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/2025/COCONUT/OpenCoconut/.venv/lib/python3.11/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.48 GiB of which 4.31 MiB is free. Process 1079651 has 22.64 GiB memory in use. Including non-PyTorch memory, this process has 820.00 MiB memory in use. Of the allocated memory 547.57 MiB is allocated by PyTorch, and 18.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "logger.info(\"Initializing model and tokenizer\")\n",
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "tokenizer.bos_token = \"<|im_start|>\"\n",
    "tokenizer.eos_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "# config and model\n",
    "coconut_config = CoconutConfig.from_tokenizer(\n",
    "    tokenizer,\n",
    "    stages=4,\n",
    "    continuous_thoughts=2,\n",
    ")\n",
    "model = AutoCoconutForCausalLM.from_pretrained(\n",
    "    config['model_name'], coconut_config, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if os.getenv(\"DEBUG\", \"0\") == \"1\":\n",
    "    model.tokenizer = tokenizer\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=config['samples_per_epoch']//config['batch_size']*config['num_epochs'],\n",
    "    logging_steps=100, # TODO ideally we log to tensorboard every step, but to ui every 100 steps\n",
    "    save_steps=10000,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    optim=\"adamw_torch\", # save memory: adamw_bnb_8bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = tokenizer.convert_tokens_to_ids('<bot>')\n",
    "tokenizer.decode([id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base checkpoint\n",
    "if not DEBUG:\n",
    "    current_output_dir = output_dir/\"stage0}\"\n",
    "    current_output_dir = current_output_dir/\"checkpoint-base\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 1\n",
    "    clear_memory()\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=256, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=1,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "    batch = next(iter(dl))\n",
    "    batch = {k: v.to(get_device()) for k, v in batch.items()}\n",
    "    print(batch.keys(), batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 2\n",
    "    for i in [0, 1, 2]:\n",
    "        model.current_stage = i\n",
    "        print(f\"Stage {i}\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            o = model(**batch, output_hidden_states=True)\n",
    "            print(o.keys())\n",
    "            model.train()\n",
    "            o = model(**batch, output_hidden_states=True)\n",
    "            print(o.keys(), o['loss'].shape)\n",
    "            print(o['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     # QC train\n",
    "#     clear_memory()\n",
    "#     dataset = CoTDataset(\n",
    "#         \"casperhansen/gsm8k_synthetic_cot\",\n",
    "#         tokenizer,\n",
    "#         max_length=256, # all less than 256, most < 128\n",
    "#         coconut_config=coconut_config,\n",
    "#         current_stage=1,\n",
    "#         split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "#     )\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize trainer\n",
    "for stage in range(coconut_config.stages):\n",
    "    clear_memory()\n",
    "    logger.info(f\"starting stage {stage}\")\n",
    "    logger.info(\"preparing dataset\")\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=256, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=stage,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    logger.info(f\"dataset size: {len(dataset)}\")\n",
    "    model.current_stage = stage\n",
    "    current_output_dir = output_dir/f\"stage{stage}\"\n",
    "    training_args.output_dir = current_output_dir\n",
    "\n",
    "    if stage == 0:\n",
    "        training_args.num_train_epochs = config['num_epochs']\n",
    "    elif stage == coconut_config.stages-2:\n",
    "        # Penultimate stage removes all the remaining language reasoning chain\n",
    "        # This handles the long-tail distribution of reasoning chains longer than 3 steps\n",
    "        dataset.include_reasoning_steps = False\n",
    "        training_args.num_train_epochs = config['num_epochs']\n",
    "    elif stage == coconut_config.stages-1:\n",
    "        # For all datasets, after the standard schedule,\n",
    "        # the model stays in the final training stage, until the 50th epoch.\n",
    "        dataset.include_reasoning_steps = True\n",
    "        training_args.num_train_epochs = config['num_epochs'] * 3\n",
    "\n",
    "    logger.info(\"starting training\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # save tokenizer to all checkpoints after training\n",
    "    for folder in os.listdir(current_output_dir):\n",
    "        if folder.startswith(\"checkpoint-\"):\n",
    "            checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "            if os.path.isdir(checkpoint_folder):\n",
    "                tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "    # save final checkpoint\n",
    "    current_output_dir = current_output_dir/\"checkpoint-final\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)\n",
    "    logger.info(f\"finished stage {stage}. Saved to {current_output_dir}\")\n",
    "\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "prompt = \"John cuts his grass to 2 inches. \" \\\n",
    "         \"It grows .5 inches per month. \" \\\n",
    "         \"When it gets to 4 inches he cuts it back down to 2 inches. \" \\\n",
    "         \"It cost $100 to get his grass cut. How much does he pay per year?\"\n",
    "\n",
    "ans = \"\"\"\n",
    "# since it starts at 2 and never gets cut below 2, we can consider only the extra growth\n",
    "growth_annual = 0.5*12\n",
    "cost_per_inch = 100/2\n",
    "cuts = growth_annual // 2 # round it down\n",
    "cost_per_year = growth_annual * cost_per_inch\n",
    "print(f\"cost per year: {cost_per_year}==300.0\")\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print('## With out thought token')\n",
    "outputs1 = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=64,\n",
    "    streamer=streamer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# add beginning of thought token?\n",
    "inputs['input_ids'], inputs['attention_mask'] = model.append_bot(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "print('\\n## With thought token')\n",
    "outputs2 = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=64,\n",
    "    streamer=streamer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_generated_answer(model_output: str, eos_token=\"<|im_end|>\"):\n",
    "    answer_prefix = \"Answer: \"\n",
    "    start_index = model_output.find(answer_prefix)\n",
    "\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    start_index += len(answer_prefix)\n",
    "    end_index = model_output.find(eos_token, start_index)\n",
    "\n",
    "    if end_index == -1:\n",
    "        return None\n",
    "\n",
    "    extracted_answer = model_output[start_index:end_index].strip()\n",
    "    return extracted_answer\n",
    "\n",
    "def pp(s):\n",
    "    s = s.replace(tokenizer.eos_token, '')\n",
    "    s = s.replace(tokenizer.bos_token, '')\n",
    "    s = s.replace(tokenizer.pad_token, '')\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    dataloader,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    max_new_tokens: int,\n",
    "    verbose = 1,\n",
    "    add_bot = False\n",
    "):\n",
    "    total_instances = 0\n",
    "    total_correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        if add_bot:\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"] = model.append_bot(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        (\n",
    "            thought_ids,\n",
    "            language_ids,\n",
    "            thought_mask,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = split_sequences(**batch, coconut_config=model.coconut_config)\n",
    "        batch_size = thought_ids.shape[0]\n",
    "        total_instances += batch_size\n",
    "\n",
    "        # Generate\n",
    "        beam_output = model.generate(\n",
    "            input_ids=thought_ids.to(model.device),\n",
    "            attention_mask=thought_mask.to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Evaluate\n",
    "        for thought_ids_batch, output_batch in zip(thought_ids, beam_output):\n",
    "            decoded_language_ids = tokenizer.decode(language_ids[0])\n",
    "            decoded_pred_text = tokenizer.decode(output_batch)\n",
    "            answer = extract_generated_answer(\n",
    "                decoded_language_ids, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            pred_answer = extract_generated_answer(\n",
    "                decoded_pred_text, eos_token=tokenizer.eos_token\n",
    "            )\n",
    "            if answer == pred_answer:\n",
    "                total_correct += 1\n",
    "            if verbose>1:\n",
    "                print(\n",
    "                    f\"Input: {pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}\\n\"\n",
    "                    f\"decoded_language_ids: {pp(decoded_language_ids)}\\n\"\n",
    "                    f\"decoded_pred_text: {pp(decoded_pred_text)}\\n\"\n",
    "                    f\"Target: {pp(answer)}\\n\"\n",
    "                    f\"Predicted: {pp(pred_answer)}\\n\"\n",
    "                )\n",
    "    if verbose>0:\n",
    "        print(\n",
    "            f\"Input: {pp(tokenizer.decode(thought_ids_batch, skip_special_tokens=True))}\\n\"\n",
    "            f\"decoded_language_ids: {pp(decoded_language_ids)}\\n\"\n",
    "            f\"decoded_pred_text: {pp(decoded_pred_text)}\\n\"\n",
    "            f\"Target: {pp(answer)}\\n\"\n",
    "            f\"Predicted: {pp(pred_answer)}\\n\"\n",
    "        )\n",
    "    accuracy = total_correct / total_instances\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "# Load data\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=max_new_tokens,\n",
    "    coconut_config=model.coconut_config,\n",
    "    current_stage=model.coconut_config.stages,\n",
    "    split=\"valid\",\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "# eval final model\n",
    "accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we explicitly start of BoThough token\n",
    "accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens, add_bot=True)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval all check\n",
    "\n",
    "checkpoints = sorted(output_dir.glob(\"stage*/*base\")) + sorted(output_dir.glob(\"stage*/*final\"))\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Loading checkpoint: {checkpoint}\")\n",
    "    model = AutoCoconutForCausalLM.from_pretrained(\n",
    "        checkpoint, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    "    ).eval()\n",
    "    model.tokenizer = tokenizer\n",
    "    accuracy = evaluate(dataloader, tokenizer, model, max_new_tokens)\n",
    "    print(f\"Checkpoint: {checkpoint}, Accuracy: {accuracy}\")\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
