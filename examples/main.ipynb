{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from opencoconut import (\n",
    "    AutoCoconutForCausalLM,\n",
    "    CoconutConfig,\n",
    "    CoTDataset,\n",
    ")\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "from loguru import logger\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Qwen/Qwen2.5-0.5B',\n",
       " 'batch_size': 5,\n",
       " 'learning_rate': 0.0001,\n",
       " 'samples_per_epoch': 1000,\n",
       " 'output_dir': './output/small',\n",
       " 'num_epochs': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_debug = {\n",
    "    # 'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    # 'model_name': 'axel-datos/qwen2.5-0.5b-instruct_MATH_lisa',\n",
    "    'model_name': 'plaguss/Qwen2.5-0.5B-Math-Shepherd-PRM-0.1',\n",
    "    'batch_size': 5,\n",
    "    'learning_rate': 4e-4,\n",
    "    'samples_per_epoch': 12,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_small = {\n",
    "    'model_name': \"Qwen/Qwen2.5-0.5B\",\n",
    "    'batch_size': 5,\n",
    "    'learning_rate': 1e-4,\n",
    "    'samples_per_epoch': 1000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 1,\n",
    "}\n",
    "config_medium = {\n",
    "    'model_name': \"Qwen/Qwen2.5-2.5B\",\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'samples_per_epoch': 30000,\n",
    "    'output_dir': \"./output/small\",\n",
    "    'num_epochs': 3,\n",
    "}\n",
    "config = config_small\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    config = config_debug\n",
    "    os.environ[\"DEBUG\"] = \"1\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 09:42:59.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mInitializing model and tokenizer\u001b[0m\n",
      "Some weights of CoconutQwen2ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B and are newly initialized: ['switch.0.weight', 'switch.1.bias', 'switch.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "logger.info(\"Initializing model and tokenizer\")\n",
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'], padding_side=\"left\")\n",
    "tokenizer.bos_token = \"<|im_start|>\"\n",
    "tokenizer.eos_token = \"<|im_end|>\"\n",
    "tokenizer.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "# config and model\n",
    "# FIXME we wont need custom tokens anymore\n",
    "coconut_config = CoconutConfig.from_tokenizer(tokenizer)\n",
    "model = AutoCoconutForCausalLM.from_pretrained(\n",
    "    config['model_name'], coconut_config, torch_dtype=torch.bfloat16, device_map=get_device(), \n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    num_train_epochs=config['num_epochs'],\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=config['samples_per_epoch']//config['batch_size']*config['num_epochs'],\n",
    "    logging_steps=20, # TODO ideally we log to tensorboard every step, but to ui every 100 steps\n",
    "    save_steps=10000,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    optim=\"adamw_torch\", # save memory: adamw_bnb_8bit adamw_bnb_8bit or PagedAdamW8bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---BEGIN---\n",
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?En el lado, 19999999999999999999999999999999999999999999999999999999999\n",
      "---END---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[13079, 15104,   806, 16359,   311,   220,    17, 14924,    13,  1084,\n",
       "         27715,   659,    20, 14924,   817,  2254,    13,  3197,   432,  5221,\n",
       "           311,   220,    19, 14924,   566, 15104,   432,  1182,  1495,   311,\n",
       "           220,    17, 14924,    13,  1084,  2783,   400,    16,    15,    15,\n",
       "           311,   633,   806, 16359,  3931,    13,  2585,  1753,  1558,   566,\n",
       "          2291,   817,  1042,    30,  1702,   655, 43424,    11,   220,    16,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24,    24,    24,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24,    24,    24,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24,    24,    24,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24,    24,    24,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24,    24,    24,\n",
       "            24,    24,    24,    24,    24,    24,    24,    24]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "prompt = \"John cuts his grass to 2 inches. \" \\\n",
    "         \"It grows .5 inches per month. \" \\\n",
    "         \"When it gets to 4 inches he cuts it back down to 2 inches. \" \\\n",
    "         \"It cost $100 to get his grass cut. How much does he pay per year?\"\n",
    "\n",
    "ans = \"\"\"\n",
    "# since it starts at 2 and never gets cut below 2, we can consider only the extra growth\n",
    "growth_annual = 0.5*12\n",
    "cost_per_inch = 100/2\n",
    "cuts = growth_annual // 2 # round it down\n",
    "cost_per_year = growth_annual * cost_per_inch\n",
    "print(f\"cost per year: {cost_per_year}==300.0\")\n",
    "\"\"\"\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# print('## With out thought token')\n",
    "# outputs1 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=256,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "\n",
    "def infer_example(model, tokenizer, prompt=prompt, ans=\"300\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    print('---BEGIN---')\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=64,\n",
    "        streamer=streamer,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    print('---END---')\n",
    "    # print(f\"Prompt: {prompt}\")\n",
    "    # print(f\"Answer: {ans}\")\n",
    "    # print(f\"Generated: {outputs[0]['generated_text']}\")\n",
    "    # print()\n",
    "    return outputs\n",
    "\n",
    "infer_example(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencoconut.evaluate import evaluate, extract_generated_answer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load val data\n",
    "dataset_val = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=256,\n",
    "    coconut_config=model.coconut_config,\n",
    "    current_stage=model.coconut_config.stages,\n",
    "    split=\"valid[:64]\" if DEBUG else \"valid\",\n",
    ")\n",
    "dataset_val[0]\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=config['batch_size'], shuffle=False)\n",
    "max_new_tokens = 6 if DEBUG else 64\n",
    "# eval final model\n",
    "# accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 1\n",
    "    clear_memory()\n",
    "    dataset = CoTDataset(\n",
    "        \"casperhansen/gsm8k_synthetic_cot\",\n",
    "        tokenizer,\n",
    "        max_length=256, # all less than 256, most < 128\n",
    "        coconut_config=coconut_config,\n",
    "        current_stage=0,\n",
    "        split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'])\n",
    "    batch = next(iter(dl))\n",
    "    batch = {k: v.to(get_device()) for k, v in batch.items()}\n",
    "    print(batch.keys(), batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # QC forward: part 2\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys())\n",
    "        model.train()\n",
    "        o = model(**batch, output_hidden_states=True)\n",
    "        print(o.keys(), o['loss'].shape)\n",
    "        print(o['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     # QC train\n",
    "#     clear_memory()\n",
    "#     dataset = CoTDataset(\n",
    "#         \"casperhansen/gsm8k_synthetic_cot\",\n",
    "#         tokenizer,\n",
    "#         max_length=256, # all less than 256, most < 128\n",
    "#         coconut_config=coconut_config,\n",
    "#         current_stage=1,\n",
    "#         split=f\"train[:{config['samples_per_epoch']}]\",\n",
    "#     )\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base checkpoint\n",
    "if not DEBUG:\n",
    "    current_output_dir = output_dir/\"stage0}\"\n",
    "    current_output_dir = current_output_dir/\"checkpoint-base\"\n",
    "    model.save_pretrained(current_output_dir)\n",
    "    tokenizer.save_pretrained(current_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 09:44:43.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mpreparing dataset\u001b[0m\n",
      "\u001b[32m2025-01-16 09:44:47.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mdataset size: 1000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---BEGIN---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay per year?En el lado, 19999999999999999999999999999999999999999999999999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-16 09:58:29.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mstarting training\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---END---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/200 01:41 < 53:59, 0.06 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize trainer\n",
    "stage = 0\n",
    "logger.info(\"preparing dataset\")\n",
    "dataset = CoTDataset(\n",
    "    \"casperhansen/gsm8k_synthetic_cot\",\n",
    "    tokenizer,\n",
    "    max_length=256, # all less than 256, most < 128\n",
    "    coconut_config=coconut_config,\n",
    "    current_stage=stage,\n",
    "    split=f\"train[:{config['samples_per_epoch']}]\",\n",
    ")\n",
    "logger.info(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "current_output_dir = output_dir/f\"stage{stage}\"\n",
    "training_args.output_dir = current_output_dir\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "class GenExCallback(TrainerCallback):\n",
    "\n",
    "    # on_save on_epoch_start on_log \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            outs = infer_example(self.model, self.model.tokenizer)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            outs = infer_example(self.model, self.model.tokenizer)\n",
    "            print(logs)\n",
    "\n",
    "\n",
    "infer_example(model, model.tokenizer)\n",
    "logger.info(\"starting training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[GenExCallback],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save tokenizer to all checkpoints after training\n",
    "for folder in os.listdir(current_output_dir):\n",
    "    if folder.startswith(\"checkpoint-\"):\n",
    "        checkpoint_folder = os.path.join(current_output_dir, folder)\n",
    "        if os.path.isdir(checkpoint_folder):\n",
    "            tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "# save final checkpoint\n",
    "current_output_dir = current_output_dir/\"checkpoint-final\"\n",
    "model.save_pretrained(current_output_dir)\n",
    "tokenizer.save_pretrained(current_output_dir)\n",
    "logger.info(f\"finished stage {stage}. Saved to {current_output_dir}\")\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# TODO add callback for gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cpu() # HACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add beginning of thought token?\n",
    "# inputs['input_ids'], inputs['attention_mask'] = model.append_bot_token(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# print('\\n## With thought token')\n",
    "# outputs2 = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     max_new_tokens=64,\n",
    "#     streamer=streamer,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from opencoconut import AutoCoconutForCausalLM, CoTDataset, split_sequences\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "model.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval final model\n",
    "accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eval all check, we want to see acc increasing from the first, which is the base model\n",
    "\n",
    "# # FIXME loading seems broken :(\n",
    "\n",
    "# checkpoints = sorted(output_dir.glob(\"stage*/*base\")) + sorted(output_dir.glob(\"stage*/*final\"))\n",
    "\n",
    "# for checkpoint in checkpoints:\n",
    "#     print(f\"Loading checkpoint: {checkpoint}\")\n",
    "#     model = AutoCoconutForCausalLM.from_pretrained(\n",
    "#         checkpoint, torch_dtype=torch.bfloat16, device_map=get_device()\n",
    "#     ).eval()\n",
    "#     model.tokenizer = tokenizer\n",
    "#     accuracy = evaluate(dataloader_val, tokenizer, model, max_new_tokens)\n",
    "#     print(f\"Checkpoint: {checkpoint}, Accuracy: {accuracy}\")\n",
    "#     clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
